{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class classification with Softmax regression\n",
    "\n",
    "- https://www.youtube.com/watch?v=MFAnsx1y9ZI&index=13&list=PLlMkM4tgfjnLSOjrEJN31gZATbcj_MpUm\n",
    "\n",
    "- multinomial classification = 3 개의 독립된 클래시파이어를 만들면 즉 각 클래서를 선택하는 클래시파이어를 만드는 것인데, 그냥 W 행렬을 1x3 짜리 3개를 만들지 말고 3x3 짜리 한 개를 만들자. 그러면, $Y=W * X$ 에서 얻어지는 $Y$는 3차원 벡터가 된다.\n",
    "- 그러면 $Y$ 값을 확률값으로 변경하고자하면 어떻게 해야하나. softmax 함수를 사용하면 된다.\n",
    "- $Softmax(y_i) = \\frac{\\exp{y_i}}{\\sum \\exp{y_k}}$\n",
    "- https://www.tensorflow.org/api_docs/python/tf/nn/softmax\n",
    "- ``log_softmax`` 가 정의되어 있다. https://www.tensorflow.org/versions/r0.10/api_docs/python/nn/classification\n",
    "- https://www.tensorflow.org/get_started/mnist/beginners\n",
    "- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py : may be numerically unstable so, ``tf.nn.softmax_cross_entropy_with_logits()`` 를 대신 사용한다.\n",
    "\n",
    "## 학습을 위한 코스트 함수는?\n",
    "- cross-entropy 를 사용하자\n",
    "- $D (S, L) = -\\sum_i L_i \\log(S_i)$, where $S$ is the prediction output vector from softmax and $L$ is the label vector (e.g. [0, 1, 0]).\n",
    "\n",
    "### Ref.\n",
    "- http://stackoverflow.com/questions/40675182/tensorflow-log-softmax-tf-nn-logtf-nn-softmaxpredict-tf-nn-softmax-cross-ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xdata:  [[ 1.  2.  1.]\n",
      " [ 1.  3.  2.]\n",
      " [ 1.  3.  4.]\n",
      " [ 1.  5.  5.]\n",
      " [ 1.  7.  5.]\n",
      " [ 1.  2.  5.]\n",
      " [ 1.  6.  6.]\n",
      " [ 1.  7.  7.]]\n",
      "ydata:  [[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "notice that each row corresponds to one data input\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('train-softmax.txt', unpack=True, dtype='float32')\n",
    "xdata = np.transpose(xy[0:3])\n",
    "ydata = np.transpose(xy[3:])\n",
    "print ('xdata: ', xdata)\n",
    "print ('ydata: ', ydata)\n",
    "print ('notice that each row corresponds to one data input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[ 0.36091304  0.2101326   0.22026467]\n",
      " [ 0.1851213  -0.57366562 -0.51584959]\n",
      " [ 0.28643727  0.12737894 -0.28107858]]\n",
      "X [[ 1.  2.  1.]\n",
      " [ 1.  3.  2.]\n",
      " [ 1.  3.  4.]\n",
      " [ 1.  5.  5.]\n",
      " [ 1.  7.  5.]\n",
      " [ 1.  2.  5.]\n",
      " [ 1.  6.  6.]\n",
      " [ 1.  7.  7.]]\n",
      "Y [[ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n",
      "softmax(X*W) [[ 0.77999818  0.12544645  0.09455537]\n",
      " [ 0.91047555  0.05848143  0.03104303]]\n",
      "hypothesis= [[ 0.77999818  0.12544645  0.09455537]\n",
      " [ 0.91047555  0.05848143  0.03104303]]\n",
      "tf.log(hypothesis)= [[-0.24846369 -2.07587624 -2.35856962]\n",
      " [-0.09378823 -2.839046   -3.47238088]]\n",
      "Y*tf.log(hypothesis)= [[-0.         -0.         -2.35856962]\n",
      " [-0.         -0.         -3.47238088]]\n",
      "tf.reduce_sum(Y*tf.log(hypothesis), axis=1)= [-2.35856962 -3.47238088]\n",
      "cost = tf.reduce_mean(-tf.reduce_sum) =  1.44017\n",
      "0/2001 cost=1.2516590356826782 \n",
      "2000/2001 cost=0.8216826915740967 \n",
      "4000/2001 cost=0.8047826290130615 \n",
      "6000/2001 cost=0.797741174697876 \n",
      "8000/2001 cost=0.7936605215072632 \n",
      "10000/2001 cost=0.7909739017486572 \n",
      "12000/2001 cost=0.7890682816505432 \n",
      "14000/2001 cost=0.7876465320587158 \n",
      "16000/2001 cost=0.7865456342697144 \n",
      "18000/2001 cost=0.7856684923171997 \n",
      "20000/2001 cost=0.7849537134170532 \n",
      "--- learning result ---\n",
      "hypothesis= [[  3.01665409e-06   1.75747132e-07   9.99996781e-01]\n",
      " [  7.48079037e-05   8.38801242e-08   9.99925137e-01]\n",
      " [  3.41500156e-02   4.06127237e-03   9.61788774e-01]\n",
      " [  5.00604510e-01   2.14133206e-06   4.99393344e-01]\n",
      " [  5.65038383e-01   8.44122168e-12   4.34961677e-01]\n",
      " [  1.52014080e-03   9.96242762e-01   2.23714788e-03]\n",
      " [  9.61330473e-01   7.91428434e-08   3.86694521e-02]\n",
      " [  9.98380661e-01   1.58191649e-09   1.61934120e-03]]\n",
      "predicted Indx= [array([2, 2, 2, 0, 0, 1, 0, 0])]\n",
      "test output:  [0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# TF Graph Input\n",
    "X = tf.placeholder (\"float\", [None,3])\n",
    "Y = tf.placeholder (\"float\", [None,3])\n",
    "\n",
    "input_dim = 3\n",
    "output_dim = 3\n",
    "W = tf.Variable(tf.random_uniform([input_dim, output_dim], -1,1))\n",
    "hypothesis = tf.nn.softmax (tf.matmul(X, W))\n",
    "\n",
    "#cross entropy\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis), axis=1))\n",
    "\n",
    "```\n",
    "cost = tf.reduce_mean( \n",
    "    tf.nn.softmax_cross_entropy_with_logits (labels=Y, logits=hypothesis))\n",
    "```\n",
    "learning_rate = 0.1\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "with tf.Session() as ss:\n",
    "    ss.run(tf.global_variables_initializer())\n",
    "    feed_dict = {X:xdata[:2], Y:ydata[:2]}\n",
    "    print ('W:', ss.run(W))\n",
    "    print ('X', ss.run(X, feed_dict = {X:xdata}))\n",
    "    print ('Y', ss.run(Y, feed_dict = {Y:ydata}))\n",
    "    print ('softmax(X*W)', ss.run(tf.nn.softmax(tf.matmul(X,W)), \n",
    "                                  feed_dict={X:xdata[:2]}))\n",
    "    print ('hypothesis=', ss.run(hypothesis, feed_dict))\n",
    "    print ('tf.log(hypothesis)=',\n",
    "          ss.run(\n",
    "          tf.log(hypothesis), feed_dict))\n",
    "    print ('Y*tf.log(hypothesis)=',\n",
    "          ss.run(Y*tf.log(hypothesis), feed_dict))\n",
    "    print ('tf.reduce_sum(Y*tf.log(hypothesis), axis=1)=',\n",
    "          ss.run(tf.reduce_sum(Y*tf.log(hypothesis), axis=1), feed_dict))\n",
    "    print ('cost = tf.reduce_mean(-tf.reduce_sum) = ', \n",
    "           ss.run(cost, feed_dict))\n",
    "\n",
    "    feed_dict = {X:xdata, Y:ydata}\n",
    "    \n",
    "    for step in range(20001):\n",
    "        ss.run(optimizer, feed_dict)\n",
    "        if step%2000==0:\n",
    "            print ('{}/2001 cost={} '.format(\n",
    "                step,\n",
    "                ss.run(cost, feed_dict))\n",
    "            )\n",
    "            \n",
    "    # see the learning result\n",
    "    print ('--- learning result ---')\n",
    "    re = ss.run(hypothesis, feed_dict)\n",
    "    rIndx = [ss.run(tf.arg_max(re,1))]\n",
    "    print ('hypothesis=', re)\n",
    "    print ('predicted Indx=', rIndx)\n",
    "    \n",
    "    testX = [ [1,11,7], [1,3,4], [1,1,0]]\n",
    "    print ('test output: ', ss.run(tf.arg_max(hypothesis,1), \n",
    "                                   feed_dict={X:testX}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "대략 돌아가는 상황을 파악하는 용도로만 사용하자. 데이터 8개로 피팅이 정말 잘 될 거라고 생각하지는 말아야하지않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
